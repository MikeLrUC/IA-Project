@misc{tan2021flying,
    title={Flying Guide Dog: Walkable Path Discovery for the Visually Impaired Utilizing Drones and Transformer-based Semantic Segmentation}, 
    author={Haobin Tan and Chang Chen and Xinyu Luo and Jiaming Zhang and Constantin Seibold and Kailun Yang and Rainer Stiefelhagen},
    year={2021},
    eprint={2108.07007},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
}

@article{modAL,
    title={mod{AL}: {A} modular active learning framework for {P}ython},
    author={Tivadar Danka and Peter Horvath},
    url={https://github.com/modAL-python/modAL},
    note={available on arXiv at \url{https://arxiv.org/abs/1805.00979}}
}

@inproceedings{AlexNet,
    author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
    title = {ImageNet Classification with Deep Convolutional Neural Networks},
    year = {2012},
    publisher = {Curran Associates Inc.},
    address = {Red Hook, NY, USA},
    abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million
    high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the
    test data, we achieved top-1 and top-5 error rates of 37.5\% and 17.0\% which is considerably better
    than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000
    neurons, consists of five convolutional layers, some of which are followed by max-pooling layers,
    and three fully-connected layers with a final 1000-way softmax. To make training faster, we used
    non-saturating neurons and a very efficient GPU implementation of the convolution operation. To
    reduce overriding in the fully-connected layers we employed a recently-developed regularization
    method called "dropout" that proved to be very effective. We also entered a variant of this model
    in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3\%, compared to
    26.2\% achieved by the second-best entry.}, 
    booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 1},
    pages = {1097–1105},
    numpages = {9},
    location = {Lake Tahoe, Nevada},
    series = {NIPS'12}
}

@misc{PVTL,
    author = {Tan, Haobin and Chen, Chang and Luo, Xinyu and Zhang, Jiaming and Seibold, Constantin and Yang, Kailun and Stiefelhagen, Rainer}, 
    title = {Pedestrian Vehicles and Traffic Lights Dataset},
    url = {https://drive.google.com/drive/folders/1UFcr-b4Ci5BsA72TZWJ77n-J3aneli6l} ,
    journal = {Google Drive},
    publisher = {Google},
    year = {2021},
    month = {9}
} 

@techreport{YY2017,
  author = {Yao-Yuan Yang and Shao-Chuan Lee and Yu-An Chung and Tung-En Wu and Si-An Chen and Hsuan-Tien Lin},
  title = {libact: Pool-based Active Learning in Python},
  institution = {National Taiwan University},
  url = {https://github.com/ntucllab/libact},
  note = {available as arXiv preprint \url{https://arxiv.org/abs/1710.00379}},
  month = oct,
  year = 2017
}

@misc{JCLAL,
  author = {Oscar Gabriel Reyes Pupo},
  title = {JCLAL},
  year = {2016},
  publisher = {GitHub},
  journal = {JCLAL GitHub repository},
  howpublished = {\url{https://github.com/ogreyesp/JCLAL}},
  commit = {545f7a86adc2cc9d0eb1bc5f2b2cbec7c8b9c8bf}
}

@article{LeNet-5,
  author={Lecun, Y. and Bottou, L. and Bengio, Y. and Haffner, P.},
  journal={Proceedings of the IEEE},
  title={Gradient-based learning applied to document recognition},
  year={1998},
  volume={86},
  number={11},
  pages={2278-2324},
  doi={10.1109/5.726791}
}

@InProceedings{pmlr-v16-settles11a,
  title = {From Theories to Queries: Active Learning in Practice},
  author = {Settles, Burr},
  booktitle = {Active Learning and Experimental Design workshop In conjunction with AISTATS 2010},
  pages = {1--18},
  year = {2011},
  editor = 	 {Guyon, Isabelle and Cawley, Gavin and Dror, Gideon and Lemaire, Vincent and Statnikov,
  Alexander},
  volume = {16},
  series = {Proceedings of Machine Learning Research},
  address = {Sardinia, Italy},
  month = {5},
  publisher = {PMLR},
  pdf = {http://proceedings.mlr.press/v16/settles11a/settles11a.pdf},
  url = {https://proceedings.mlr.press/v16/settles11a.html},
  abstract = {This article surveys recent work in active learning aimed at making it more practical for
  real-world use. In general, active learning systems aim to make machine learning more economical,
  since they can participate in the acquisition of their own training data. An active learner might
  iteratively select informative query instances to be labeled by an oracle, for example. Work over
  the last two decades has shown that such approaches are effective at maintaining accuracy while
  reducing training set size in many machine learning applications. However, as we begin to deploy
  active learning in real ongoing learning systems and data annotation projects, we are encountering
  unexpected problems–due in part to practical realities that violate the basic assumptions of earlier
  foundational work. I review some of these issues, and discuss recent work being done to address the
  challenges.}
}

@techreport{settles.tr09,
    author = {Burr Settles},
    institution = {University of Wisconsin--Madison},
    number = {1648},
    title = {Active Learning Literature Survey},
    type = {Computer Sciences Technical Report},
    year = {2009}
}

@misc{PIL,
  title={Pillow (PIL Fork)},
  author={Clark, Alex},
  year={2015},
  publisher={readthedocs},
  url={https://github.com/python-pillow/Pillow}
}

@misc{openCV,
    author = {Bradski, G.},
    journal = {Dr. Dobb's Journal of Software Tools},
    title = {{The OpenCV Library}},
    year = {2000},
    url = {https://pypi.org/project/opencv-python/}
}

@misc{tensorflow,
  title={Tensorflow: A system for large-scale machine learning},
  author={Abadi, Mart{\'i}n and Barham, Paul and Chen, Jianmin and Chen, Zhifeng and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Irving, Geoffrey and Isard, Michael and others},
  year={2016},
  url = {https://www.tensorflow.org/lite/guide/python}
}



